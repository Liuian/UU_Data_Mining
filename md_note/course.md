## (OK) 37A 37B Classification Trees
### (OK) (37A P.32) How many possible binary split for a categorical attribute?
$2^{L - 1} - 1$
- L: distinct values  

å› ç‚ºä¸€å€‹é›†åˆå¯èƒ½çš„éç©ºï¼‹éå…¨éƒ¨subsetsæœ‰$2^{L-2}$å€‹ï¼Œæœƒå…©å…©å°æ‡‰ï¼ˆè£œé›†complementï¼‰ï¼Œå› æ­¤è¦é™¤ä»¥2ï¼Œç­‰æ–¼$2^{L - 1} - 1$ã€‚

### (OK) (37A P.33,34) How many optimal binary split for a categorical attribute?  
exc1- question1 - b  
ç®—å‡ºæ‰€æœ‰categorieså‡ºç¾æŸä¸€å€‹classçš„æ©Ÿç‡ï¼Œæ’åºé€™å€‹æ©Ÿç‡ï¼Œåªå¾æ’åºçš„ä¸­é–“åˆ‡æ–·ä»–å€‘ã€‚  

Example:
1. å…ˆç®—conditional probability å° class 0
    P(0|x=a), P(0|x=b), P(0|x=c), P(0|x=d)
2. æ ¹æ“šæ©Ÿç‡æ’åº
    c<b<a<d
3. ä¸­é–“çš„æ–·é»å°±æ˜¯optimal binary split
    {c},{c,b},{c,b,a}


### (OK) (37A P.31) How many possiable binary split for a numeric attributes?  
exc1 - question1 - c  
Ans: $n-1$  
è‹¥æœ‰$n$å€‹ä¸åŒçš„æ•¸å€¼ï¼Œå¯èƒ½çš„åˆ‡é»å°±æ˜¯$n-1$å€‹ã€‚

### (OK) (37A P.35~37) How many optimal binary split for a numeric attributes?
exc1 - question1 - d  
å…ˆæŠŠæ¯å€‹æ•¸å€¼æ’åºï¼Œè¨ˆç®—é€™å€‹æ•¸å€¼åœ¨æŸå€‹classçš„conditionsl probability (P(A|x=28), P(A|x=31)...)ï¼Œç„¶å¾Œçœ‹åœ¨æ•¸å€¼çš„æ’åºåº•ä¸‹ï¼Œprobabilityè¢«åˆ‡æˆå¹¾æ®µã€‚ï¼ˆæœ‰å¯èƒ½é€£çºŒå¥½å¹¾å€‹æ•¸å€¼çš„æ©Ÿç‡æ˜¯ä¸€æ¨£çš„ï¼Œé‚£ä¸­é–“å°±ä¸éœ€è¦åˆ‡é»ï¼‰  

Example:
|          |  8  |  12 |  14 |  16 |  18 |  20 |
| :------- | :-: | :-: | :-: | :-: | :-: | :-: |
| **P(A)** | 0.5 | 0.5 | 1.0 | 1.0 | 1.0 | 0.5 |
| **P(B)** | 0.5 | 0.5 | 0.0 | 0.0 | 0.0 | 0.5 |

### (OK) Gini-Index and Impurity Reduction
exc1 - question1 - e  

Gini-Index ç”¨ä¾†è¡¡é‡ä¸€å€‹ç¯€é»çš„æ··äº‚ç¨‹åº¦ã€‚è‹¥ä¸€å€‹ç¯€é»å®Œå…¨å±¬æ–¼åŒä¸€å€‹é¡åˆ¥ï¼Œå‰‡gini-indexç‚º0ã€‚

- Gini-Index formula: 
    $$
    i(t) = P(0 \mid t) , P(1 \mid t)
    $$
    æˆ–ç­‰åƒ¹åœ°å¯«æˆï¼š
    $$
    i(t) = P(0 \mid t) [1 - P(0 \mid t)]
    $$
    * $P(0 \mid t)$ï¼šç¯€é» $t$ ä¸­å±¬æ–¼é¡åˆ¥ 0 çš„æ©Ÿç‡
    * $P(1 \mid t)$ï¼šç¯€é» $t$ ä¸­å±¬æ–¼é¡åˆ¥ 1 çš„æ©Ÿç‡

    **General Formula:**
    $$
    I(t) = 1 - \sum_{j=1}^{k} [P(j \mid t)]^2
    $$
    * $I(t)$ï¼šç¯€é» (t) çš„ Gini ä¸ç´”åº¦
    * $k$ï¼šé¡åˆ¥ï¼ˆclassï¼‰çš„ç¸½æ•¸
    * $P(j \mid t)$ï¼šåœ¨ç¯€é» (t) ä¸­ï¼Œå±¬æ–¼é¡åˆ¥ (j) çš„æ¨£æœ¬æ¯”ä¾‹

- Impurity Reduction ï¼ˆä¸ç´”åº¦é™ä½ï¼‰:  
    ç•¶ä¸€å€‹ç¯€é» ( t ) è¢«åˆ†è£‚æˆå·¦å­ç¯€é» ( \ell ) å’Œå³å­ç¯€é» ( r ) æ™‚ï¼Œæˆ‘å€‘å¯ä»¥è¨ˆç®—é€™æ¬¡åˆ†è£‚å¾Œã€Œä¸ç´”åº¦é™ä½äº†å¤šå°‘ã€ã€‚é€™å€‹å€¼å¯ä»¥ç”¨ä¾†åˆ¤æ–·ã€Œåˆ†è£‚æ˜¯å¦æœ‰æ•ˆã€ã€‚
    $$
    \Delta i(S, t) = i(t) - [\pi(\ell) i(\ell) + \pi(r) i(r)]
    $$
    * $i(t)$ï¼šæ¯ç¯€é»ï¼ˆåˆ†è£‚å‰ï¼‰çš„ Gini ä¸ç´”åº¦
    * $i(\ell), i(r)$ï¼šå·¦å³å­ç¯€é»çš„ Gini ä¸ç´”åº¦
    * $\pi(\ell), \pi(r)$ï¼šå·¦å³å­ç¯€é»æ¨£æœ¬æ•¸ä½”æ•´é«”æ¨£æœ¬çš„æ¯”ä¾‹

è¨ˆç®—æ¯ç¨®ä¸åŒçš„ split çš„ impurity reductionï¼Œä»¥æ‰¾å‡ºæœ€å¥½çš„ split æ–¹æ³•ï¼Œæ­¥é©Ÿå¦‚ä¸‹ï¼š
1. ç•«å‡ºé‚„æ²’ split ä¹‹å‰çš„ nodeï¼Œnodeè£¡é¢å¯«å‡ºä¸åŒclassçš„æ¯”ä¾‹ï¼Œæ¥è‘—å†å‘ä¸‹å·¦å³åˆ†å‡ºå…©å€‹ nodesï¼Œnodeè£¡é¢ä¸€æ¨£å¯«è©²nodeæ‰€æœ‰è§€å¯Ÿå€¼ä¸åŒclassçš„æ¯”ä¾‹ï¼Œå‘ä¸‹åˆ†æ”¯çš„å…©æ¢ç·šè¦å¯«åˆ†éå»çš„è§€å¯Ÿå€¼çš„æ¯”ä¾‹ã€‚
2. è¨ˆç®—æ¯ä¸€å€‹nodesçš„gini-indexã€‚
3. åˆ©ç”¨ä»¥ä¸Šæ‰€æœ‰çš„è³‡è¨Šè¨ˆç®—impurity reductionã€‚
4. æ‰¾å‡ºæœ€å¤§çš„impurity reductionï¼Œè©²splitå°±æ˜¯æœ€å¥½çš„åˆ†æ³•ã€‚

### (OK) (37B P.13,17) Total **cost of a tree** & **Resubstitution error**
exc1 - question3
- Formula
    $$
    C_{\alpha}(T) = R(T) + \alpha |\tilde{T}|
    $$

- ç¸½æˆæœ¬çš„åˆ†è§£ â€” Decomposition of Total Cost
    $$
    C_{\alpha}(T) = R(T) + \alpha |\tilde{T}| = \sum_{t \in \tilde{T}} \big[ R(t) + \alpha \big]
    $$

- Resubstitution error
$$
R(T) = \frac{\text{Number of wrong classifications made by } T}{\text{Number of examples in the training sample}}
$$

- å°å–®ä¸€çµ‚ç«¯ç¯€é»çš„R(t)ï¼š$\frac{\text{é‚£ä¸€å€‹ç¯€é»è‡ªå·±çš„éŒ¯èª¤åˆ†é¡æ•¸é‡}}{\text{æ•´æ£µæ¨¹çš„æ¨£æœ¬æ•¸é‡}}$

* $R(T)$ï¼šè¡¡é‡æ¨¡å‹ã€Œ**æœ‰å¤šæº–ç¢º**ã€
* $|\tilde{T}|$ï¼šè¡¡é‡æ¨¡å‹ã€Œ**æœ‰å¤šè¤‡é›œ**ã€
* $\alpha$ï¼šèª¿æ•´é€™å…©è€…çš„æ¬Šé‡
* $C_{\alpha}(T)$ï¼šç¶œåˆè€ƒæ…®èª¤å·®èˆ‡è¤‡é›œåº¦çš„ã€Œ**ç¸½æˆæœ¬**ã€ï¼Œå‰ªææ™‚è¦æ‰¾åˆ°è®“å®ƒæœ€å°çš„æ¨¹ã€‚
* $t$ï¼šå–®ä¸€çµ‚ç«¯ç¯€é»ï¼ˆleaf nodeï¼‰



**å…¬å¼çš„ä½œç”¨èˆ‡ç›´è¦ºèªªæ˜ï¼š**
é€™å€‹å…¬å¼çš„ç›®çš„æ˜¯è¦åœ¨ã€Œ**æ¨¡å‹çš„æº–ç¢ºç‡**ã€èˆ‡ã€Œ**æ¨¡å‹çš„ç°¡å–®æ€§**ã€ä¹‹é–“æ‰¾åˆ°å¹³è¡¡ã€‚

* ç¬¬ä¸€é … $R(T)$ï¼šè¶Šå°è¡¨ç¤ºè¶Šæº–ç¢º
* ç¬¬äºŒé … $\alpha |\tilde{T}|$ï¼šè¶Šå°è¡¨ç¤ºæ¨¡å‹è¶Šç°¡å–®
* å› æ­¤ï¼š

* è‹¥ $\alpha = 0$ï¼šåªé‡è¦–æº–ç¢ºç‡ â†’ æ¨¡å‹å®¹æ˜“**éæ“¬åˆï¼ˆoverfittingï¼‰**
* è‹¥ $\alpha$ å¾ˆå¤§ï¼šé‡æ‡²è¤‡é›œåº¦ â†’ æ¨¡å‹æœƒ**è¢«å¼·è¿«è®Šç°¡å–®ï¼ˆunderfittingï¼‰**

ğŸ‘‰ æœ€ä½³çš„å‰ªæå°±æ˜¯æ‰¾åˆ°èƒ½æœ€å°åŒ– $C_{\alpha}(T)$ çš„é‚£æ£µæ¨¹ã€‚













### (37B P.22) (OK) Cirtical alpha value for pruning
- æ­¥é©Ÿ
    1. è¨ˆç®—æ¯å€‹ä¸æ˜¯leaf node çš„ critical alpha value
    2. æ‰¾åˆ°æœ€å°çš„é‚£ä¸€å€‹ï¼Œä¸¦ä¸”å°è©² node åš prune
    3. å¾—åˆ°ä¸€å€‹æ–°çš„ prune éçš„æ¨¹ï¼Œé‡è¤‡æ­¥é©Ÿ1, 2ç›´åˆ°å‰©ä¸‹ root node

- g(t) æ˜¯ä»€éº¼
g(t) æ˜¯ç¯€é» t çš„ Î± è‡¨ç•Œå€¼ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå¦‚æœ Î± å€¼å¤§æ–¼ g(t) é‚£éº¼å‰ªæ‰é€™å€‹å­æ•¸æœƒæ¯”è¼ƒåˆ’ç®—ï¼ˆcost æ¯”è¼ƒå°ï¼‰ã€‚å®ƒè¡¡é‡äº†**æ¯åˆªä¸€å€‹è‘‰å­æœƒå¢åŠ å¤šå°‘èª¤å·®**ã€‚  
 $$
g(t) = \frac{\text{å‰ªæ‰å­æ¨¹é€ æˆçš„èª¤å·®å¢åŠ }}{\text{å‰ªæ‰å­æ¨¹ç¯€çœçš„è‘‰å­æ•¸}}
$$



- å…¬å¼
$$
g(t) = \frac{R(t) - R(T_t)}{|\tilde{T_t}| - 1}
$$

|       ç¬¦è™Ÿ      | åç¨±   | æ„æ€                 |   
| :-----------: | ------- | --------- | 
|      $t$ | éçµ‚ç«¯ç¯€é»ï¼ˆnon-terminal nodeï¼‰    | æˆ‘å€‘æ­£åœ¨è€ƒæ…®æ˜¯å¦è¦å‰ªæ‰çš„é‚£å€‹ç¯€é»ã€‚  |         
|     $T_t$     | ä»¥ç¯€é» (t) ç‚ºæ ¹çš„**å­æ¨¹ï¼ˆsubtreeï¼‰**      | åŒ…å«ç¯€é» (t) åŠå…¶æ‰€æœ‰å¾Œä»£ç¯€é»ã€‚ |          
|    $R(T_t)$   | å­æ¨¹ (T_t) çš„æ•´é«”è¨“ç·´èª¤å·®ï¼ˆresubstitution errorï¼‰ | æ²’æœ‰è¢«å‰ªæå‰çš„èª¤å·®ã€‚    |            
|     $R(t)$    | è‹¥å°‡æ•´å€‹å­æ¨¹ (T_t) å‰ªæ‰ã€æ”¹æˆä¸€å€‹å–®ä¸€è‘‰ç¯€é» (t) æ™‚çš„èª¤å·®  | å‰ªæå¾Œè©²ç¯€é»çš„èª¤å·®ã€‚   | 



### <mark>(37B P.15) (OK) What is **smallest minizing subtree**</mark>
exc1 - question3 - e

çµ¦å®šä¸€å€‹ value Î± ï¼Œåœ¨é€™å€‹å€¼åº•ä¸‹costæœ€å°çš„å‰ªææ¨¹ï¼Œå¯èƒ½æœ‰å¤šå€‹ç›¸åŒæœ€å°costçš„å‰ªææ¨¹ï¼Œè¦æ‰¾æœ€å°çš„é‚£ä¸€é¡†ã€‚

> **Definition (Smallest Minimizing Subtree)**
> For any value of $\alpha$, there exists a smallest minimizing subtree $T(\alpha)$ of $T_{\max}$ that satisfies:
>
> 1. $T(\alpha)$ minimizes the total cost for that value of $\alpha$:
>    $$
>    C_{\alpha}(T(\alpha)) = \min_{T \leq T_{\max}} C_{\alpha}(T)
>    $$
>
> 2. $T(\alpha)$ is a pruned subtree of all trees that minimize the total cost:
>    $$
>    \text{if } C_{\alpha}(T) = C_{\alpha}(T(\alpha)) \text{ then } T(\alpha) \leq T
>    $$
>
> Here, $T' \leq T$ means that $T'$ is a **pruned subtree** of $T$.

* **æœ€å¤§æ¨¹ (T_{\max})**ï¼šæ˜¯å¾è¨“ç·´è³‡æ–™ç”Ÿæˆçš„**å®Œæ•´æ±ºç­–æ¨¹**ï¼ˆæœªå‰ªæã€æœ€è¤‡é›œçš„æ¨¹ï¼‰ã€‚
* **(T(\alpha))**ï¼šæ˜¯å°æ‡‰æŸå€‹æ‡²ç½°åƒæ•¸ (\alpha) ä¸‹ï¼Œèƒ½**æœ€å°åŒ–æˆæœ¬å‡½æ•¸ (C_{\alpha}(T))** çš„æ¨¹ã€‚
* ã€Œ**smallest minimizing subtree**ã€æ„æ€æ˜¯ï¼š
  åœ¨æ‰€æœ‰èƒ½é”åˆ°æœ€å°æˆæœ¬çš„æ¨¹ä¸­ï¼Œé¸å‡º**æœ€å°çš„ä¸€æ£µ**ï¼ˆä¹Ÿå°±æ˜¯è¢«å‰ªå¾—æœ€å¤šçš„é‚£ä¸€æ£µï¼‰ã€‚

## <mark>**38A** Tree ensembles: Bagging, Boosting and Random Forests</mark>

## (OK) 38B 39A Text Clessification - Naive Bayes & Logistic Regression

### (OK) (38B P.12~20) Text Classification - Multinomial naive bayes
1. Find the Vocabulary
    æ‰¾å‡ºæ‰€æœ‰åœ¨ **training documents** ä¸­å‡ºç¾éçš„ **ä¸é‡è¤‡å–®å­—**ã€‚
    é€™å€‹é›†åˆç¨±ç‚º **vocabulary**ï¼š
    $$
    V = { w_1, w_2, \dots, w_{|V|} }
    $$

2. è¨ˆç®—ä¸‰å€‹æ•¸å€¼
    1. **Vocabulary å¤§å°**
    $$
    |V| = \text{number of unique words in all training documents}
    $$

    2. **Class 1 çš„æ‰€æœ‰ documents çš„å­—æ•¸ï¼ˆå«é‡è¤‡ï¼‰**
    $$
    N_1 = \sum_{d \in \text{class 1}} \text{count(all words in } d)
    $$

    3. **Class 2 çš„æ‰€æœ‰ documents çš„å­—æ•¸ï¼ˆå«é‡è¤‡ï¼‰**
    $$
    N_2 = \sum_{d \in \text{class 2}} \text{count(all words in } d)
    $$

3. Laplace Smoothing æ©Ÿç‡ä¼°è¨ˆå…¬å¼
    å°æ–¼æ¯å€‹å–®å­— $w_i$ å’Œæ¯å€‹é¡åˆ¥ $c$ï¼Œä½¿ç”¨ **Laplace smoothing** çš„æ©Ÿç‡ä¼°è¨ˆå…¬å¼å¦‚ä¸‹ï¼š

    $$
    \hat{P}(w_i \mid c) =
    \frac{\text{count}(w_i, c) + 1}
    {\sum_{w_j \in V} \text{count}(w_j, c) + |V|}
    $$
    * $\text{count}(w_i, c)$ï¼šå–®å­— (w_i) åœ¨æ‰€æœ‰å±¬æ–¼é¡åˆ¥ (c) çš„æ–‡ä»¶ä¸­å‡ºç¾çš„æ¬¡æ•¸
    * $|V|$ï¼švocabulary çš„å¤§å°
    * åˆ†æ¯çš„ $\sum_{w_j \in V} \text{count}(w_j, c)$ï¼šé¡åˆ¥ (c) çš„æ‰€æœ‰å­—è©ç¸½æ•¸
    * åŠ ä¸Š (+1) èˆ‡ (+|V|) æ˜¯ **Laplace smoothing**ï¼Œé¿å…æ©Ÿç‡ç‚º 0
    
    æ–‡å­—æ•˜è¿°æ˜¯ï¼š
    $$
    \hat{P}(w_i \mid c) =
    \frac{(å–®å­—W_iåœ¨æ‰€å±¬é¡åˆ¥å‡ºç¾çš„æ¬¡æ•¸) + 1}
    {é¡åˆ¥Cçš„æ‰€æœ‰å­—è©ç¸½æ•¸ + vocabulary çš„å¤§å°}
    $$

    Note: General çš„æ©Ÿç‡ä¼°è¨ˆå…¬å¼
    $$
    P(w \mid c) = \frac{N_{w,c} + \alpha}{N_c + \alpha |V|}
    $$
    $\alpha = 1$ æ™‚å°±æ˜¯ä½¿ç”¨Laplace smoothing



4. ä¼°è¨ˆ Class Prior Probabilities  
    æ–‡å­—æ•˜è¿°ï¼š
    $$
    \hat{P}(c) =
    \frac{é€™å€‹classçš„æ–‡ä»¶ç¸½æ•¸}
    {æ‰€æœ‰æ–‡ä»¶ç¸½æ•¸}
    $$
    å…¬å¼ï¼š
    $$
    \hat{P}(c) =
    \frac{N_c}{N_{\text{doc}}}
    $$
    * $N_c$ï¼šå±¬æ–¼é¡åˆ¥ $c$ çš„æ–‡ä»¶æ•¸é‡
    * $N_{\text{doc}}$ï¼šæ‰€æœ‰æ–‡ä»¶çš„ç¸½æ•¸

5. çµ¦å®šä¸€å€‹æ–°çš„documentï¼Œé æ¸¬ä»–å°æŸå€‹classçš„æ©Ÿç‡
    æœƒæœ‰å…©å€‹æ­¥é©Ÿï¼Œç¬¬ä¸€å€‹æ­¥é©Ÿå…ˆè¨ˆç®—å°æ–°æ–‡ä»¶çš„å¾Œé©—åˆ†æ•¸ï¼Œç¬¬äºŒå€‹æ­¥é©Ÿåˆ©ç”¨å‰é¢çš„çµæœç®—å‡ºæ­£è¦åŒ–å¾Œé©—æ©Ÿç‡
    1. Multinomial Naive Bayes çš„æœªæ­£è¦åŒ–åˆ†æ•¸ï¼ˆPosterior æœªæ­£è¦åŒ–ï¼‰
        $$
        \text{score}(c \mid d) \propto P(c) \prod_{w \in V} [P(w \mid c)]^{tf(w, d)}
        $$
        * $c$ï¼šé¡åˆ¥ï¼ˆclassï¼‰
        * $d$ï¼šæ¸¬è©¦æ–‡ä»¶é›†åˆï¼ˆdocumentï¼‰
        * $V$ï¼šè©å½™è¡¨ï¼ˆvocabularyï¼‰
        * $w$ï¼šå–®å­—ï¼ˆwordï¼‰
        * $tf(w, d)$ï¼šå–®å­— $w$ åœ¨æ–‡ä»¶ $d$ ä¸­çš„å‡ºç¾æ¬¡æ•¸  
        
        æ–‡å­—æ•˜è¿°æ˜¯ï¼š
        $$
        \text{score}(c \mid d) \propto \text{(é€™å€‹ class åœ¨ training document å‡ºç¾çš„æ©Ÿç‡)}\prod_{æ–°æ–‡ä»¶çš„æ¯ä¸€å€‹å–®å­—}\text{(è©²å–®å­—åœ¨trainingè³‡æ–™è£¡é¢æ­¤é¡åˆ¥çš„æ©Ÿç‡ä¼°è¨ˆ)}
        $$


    2. æ­£è¦åŒ–å¾Œçš„å¾Œé©—æ©Ÿç‡ï¼ˆNormalized Posterior Probabilityï¼‰
        $$
        \hat{P}(c \mid d) =
        \frac{\text{score}(c \mid d)}
        {\sum_{c'} \text{score}(c' \mid d)}
        $$
        æ–‡å­—æ•˜è¿°æ˜¯ï¼š
        $$
        \hat{P}(c \mid d) =
        \frac{\text(testingæ–‡ä»¶å°æˆ‘å€‘æ„Ÿèˆˆè¶£çš„classçš„åˆ†æ•¸)}
        {\sum_{æ‰€æœ‰class}\text{(testingæ–‡ä»¶å°è©²classçš„åˆ†æ•¸)}}
        $$

### (OK) Logistic Regression
åƒè€ƒ exc2 - problem3
1. æ¨¡å‹æ¦‚å¿µ

    Logistic Regression æ˜¯ä¸€ç¨®**ç·šæ€§åˆ†é¡æ¨¡å‹**ï¼Œç”¨ä¾†é æ¸¬æŸäº‹ä»¶ç™¼ç”Ÿçš„æ©Ÿç‡ã€‚
    æ¨¡å‹å‡è¨­è¼¸å…¥è®Šæ•¸èˆ‡å°æ•¸å‹ç‡ï¼ˆlog-oddsï¼‰ä¹‹é–“æ˜¯**ç·šæ€§é—œä¿‚**ï¼š

    $$
    \text{logit}(P(Y=1)) =
    \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k
    $$

    è½‰æ›æˆæ©Ÿç‡å½¢å¼ï¼š

    $$
    P(Y=1) =
    \frac{\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k)}
    {1 + \exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k)}
    $$

2. æ¨¡å‹ä¸­çš„åƒæ•¸æ„ç¾©
    * **$\beta_0$**ï¼ˆInterceptï¼‰  
    è¡¨ç¤ºç•¶æ‰€æœ‰è®Šæ•¸ç‚º 0 æ™‚ï¼Œäº‹ä»¶ç™¼ç”Ÿçš„åŸºæœ¬æ©Ÿç‡ï¼ˆæˆ– baseline oddsï¼‰ã€‚  
    åœ¨ç‰¹å®šæ‡‰ç”¨ä¸­ï¼Œå®ƒå¸¸ä»£è¡¨æŸç¨®ã€Œå›ºå®šå„ªå‹¢ã€æˆ–ã€ŒåŸºæº–æƒ…æ³ã€ã€‚

    * **$\beta_i$**ï¼ˆCoefficient for $x_i$ï¼‰  
    è¡¨ç¤ºè®Šæ•¸ $x_i$ å° log-odds çš„å½±éŸ¿ç¨‹åº¦ã€‚  
    è‹¥ $\beta_i > 0$ï¼Œå‰‡ $x_i$ å¢åŠ æœƒæé«˜äº‹ä»¶ç™¼ç”Ÿçš„æ©Ÿç‡ï¼›  
    è‹¥ $\beta_i < 0$ï¼Œå‰‡æœƒé™ä½äº‹ä»¶ç™¼ç”Ÿçš„æ©Ÿç‡ã€‚

3. è§£é¡Œçš„**æ¯”è¼ƒèˆ‡åˆ¤æ–·**
   * è‹¥ (P > 0.5)ï¼šé æ¸¬å±¬æ–¼æ­£é¡ï¼ˆä¾‹å¦‚ã€Œè´ã€ï¼‰ã€‚
   * è‹¥ (P < 0.5)ï¼šé æ¸¬å±¬æ–¼è² é¡ï¼ˆä¾‹å¦‚ã€Œè¼¸ã€ï¼‰ã€‚

4. ç·šæ€§åˆ†é¡è¦å‰‡ï¼ˆLinear Classification Ruleï¼‰
    å› ç‚º logistic function åœ¨ (P=0.5) æ™‚ï¼Œ(z=0)ï¼Œ
    æ‰€ä»¥åˆ†é¡é‚Šç•Œç‚ºï¼š

    $$
    \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k = 0
    $$

    å°æ‡‰çš„åˆ†é¡è¦å‰‡ï¼š

    $$
    \begin{cases}
    \text{Predict } Y=1, & \text{if } \beta_0 + \sum_i \beta_i x_i > 0 \
    \text{Predict } Y=0, & \text{if } \beta_0 + \sum_i \beta_i x_i < 0
    \end{cases}
    $$

5. ä¿‚æ•¸åˆç†æ€§æª¢æŸ¥ï¼ˆSign Interpretationï¼‰

    æª¢æŸ¥æ¯å€‹ (\beta_i) çš„ç¬¦è™Ÿæ˜¯å¦ç¬¦åˆå¸¸ç†ï¼š

    * è‹¥è®Šæ•¸æ‡‰è©²å¢åŠ å‹ç‡ â†’ $\beta_i > 0$
    * è‹¥è®Šæ•¸æ‡‰è©²é™ä½å‹ç‡ â†’ $\beta_i < 0$

### Note
- exp(x) = $e^x$ (exc2 - question3)


## (OK) 39B 40A 40B Undirected Graphs
### (OK) (39B P.35~40) What is U-terms? 
1. èƒŒæ™¯ï¼šå¾ Joint Distribution é–‹å§‹  
    åœ¨ **ç„¡å‘åœ–æ¨¡å‹ (Markov Random Field, MRF)** æˆ– **log-linear model** è£¡ï¼Œæˆ‘å€‘é€šå¸¸æŠŠè¯åˆæ©Ÿç‡ $P(X_1, X_2, X_3)$ å¯«æˆæŒ‡æ•¸å½¢å¼ï¼ˆlog-linear formï¼‰ï¼š
    $$
    \log P(x_1, x_2, x_3) = u_0 + u_1 x_1 + u_2 x_2 + u_3 x_3 + u_{12} x_1x_2 + u_{13} x_1x_3 + u_{23} x_2x_3 + u_{123} x_1x_2x_3
    $$
    é€™äº› $u$-terms å°±æ˜¯ä½ çœ‹åˆ°çš„ **u-terms**ã€‚

2. æ¯å€‹ $u$-term çš„å«ç¾©
    | ç¬¦è™Ÿ                         | åç¨±                 | ä»£è¡¨çš„é—œä¿‚                    |
    | :------------------------- | :----------------- | :----------------------- |
    | $u_1, u_2, u_3$          | main effects       | å„è®Šæ•¸å–®ç¨å° log æ©Ÿç‡çš„å½±éŸ¿         |
    | $u_{123}$                | 3-way interaction  | ä¸‰è®Šæ•¸ä¹‹é–“çš„å…±åŒäº¤äº’ä½œç”¨ï¼ˆä¸‰è§’å½¢ cliqueï¼‰ |

    æ‰€ä»¥åœ¨ç„¡å‘åœ–ä¸­ï¼Œ**ä¸€å€‹éé›¶çš„ $u_{ij}$** ä»£è¡¨ç¯€é» $i$ å’Œ $j$ ä¹‹é–“å­˜åœ¨ä¸€æ¢é‚Šã€‚**å¦‚æœ $u_{ij} = 0$**ï¼Œå°±ä»£è¡¨é€™å…©å€‹è®Šæ•¸ä¹‹é–“åœ¨åœ–ä¸­æ˜¯æ²’æœ‰ç›´æ¥é€£ç·šçš„ï¼ˆå¯èƒ½ conditionally independentï¼‰ã€‚

3. å¦‚ä½•å¾ä¸€å€‹ undirected graphs æ‰¾åˆ° u-terms  
ç©ºé›†åˆ + ï¼ˆæ‰€æœ‰å–®ä¸€çš„nodes - é€™å€‹å…¶å¯¦å·²ç¶“åŒ…å«åœ¨æ‰€æœ‰cliquesçš„subsetsè£¡é¢äº†ï¼‰ + æ‰€æœ‰çš„cliquesä»¥åŠä»–å€‘çš„æ‰€æœ‰éç©ºsubsetsã€‚  

ï¼ˆæˆ‘æ‰¾ä¸åˆ°åœ¨èª²æœ¬çš„å“ªè£¡ï¼‰

### (OK) (40A40B P.8,11)Fitted count
HW2 - question 2
exc3 - question 3
| æ¨¡å‹é¡å‹ | å‡è¨­ | Fitted probability | Fitted count |
| ------- | ----- | ------- | --------- |
| **Saturated model** | ç„¡é™åˆ¶ | $\displaystyle \hat{P}(G,E)=\frac{n(G,E)}{n}$ | $\displaystyle \hat{n}(G,E)=n(G,E)$ |
| **Independence model** | $G \perp E$ | $\displaystyle \hat{P}(G,E)=\frac{n(G)}{n}\cdot\frac{n(E)}{n}$ | $\displaystyle \hat{n}(G,E)=\frac{n(G),n(E)}{n}$ |


### (OK) (40A40B P.39) What is **running intersection properity (RIP)** and its **maximum likelihood fitted counts**? 
1. å…ˆæ‰¾å‡ºæ‰€æœ‰çš„cliques
2. æˆ‘å€‘éœ€è¦å¹«é€™äº›cliquesæ’åº
3. æ’åºæ–¹æ³•æ˜¯ï¼šå‰é¢æ‰€æœ‰çš„cliques unionï¼Œå†è·Ÿç¾åœ¨é€™å€‹clique interset ç”¢ç”Ÿçš„setï¼ˆé€™å€‹setå°±æ˜¯é€™å€‹cliqueçš„seperatorï¼‰ï¼Œè¦æ˜¯å‰é¢ä»»æ„è‡³å°‘ä¸€å€‹cliqueçš„subsetã€‚

è¦æ‰¾å¾—åˆ°é€™å€‹orderæ‰æ˜¯RIP(Running Intersection Properity)ã€‚

An ordering C1, . . . , Cm of the cliques of the graph has the running intersection property (RIP) iff:
$$
p.39 
$$
for some i < j, and for all j = 2, . . . , m.

We define the corresponding separator sets
$$
p.39
$$
with S1 = null.

æ‰€æœ‰çš„cliquesæ”¾åˆ†æ¯ï¼Œseperatoræ”¾åˆ†å­ï¼Œå°±æ˜¯é€™å€‹åœ–çš„maximum likelihood fitted countsã€‚

åƒè€ƒæŠ•å½±ç‰‡42é ã€‚

### (OK) (40A40B P.43~53) What is general formula for **Iterative Proportional Fitting (IPF)**?  
exc 3 - problem - c

1. IPF æ˜¯ä»€éº¼ï¼Ÿ  
Iterative Proportional Fitting (IPF) æ˜¯ä¸€å€‹ç”¨ä¾†**èª¿æ•´è¯åˆæ©Ÿç‡è¡¨ï¼ˆjoint probability table**çš„æ–¹æ³•ï¼Œç›®çš„æ˜¯è®“æ¨¡å‹åœ¨æ‰€æœ‰ clique çš„é‚Šéš›åˆ†å¸ƒï¼ˆmarginsï¼‰ ä¸Šç¬¦åˆè§€å¯Ÿè³‡æ–™ï¼ˆObserved Marginsï¼‰ã€‚  
æ›å¥è©±èªªï¼šIPF æœƒåè¦†èª¿æ•´æ©Ÿç‡è¡¨çš„å€¼ï¼Œç›´åˆ°ã€Œå°æ‡‰æ–¼ cliques çš„é‚Šéš›ã€éƒ½ç­‰æ–¼è§€å¯Ÿè³‡æ–™çš„é‚Šéš›ã€‚  
é€™æ¨£çš„æœ€çµ‚åˆ†å¸ƒå°±æ˜¯æ»¿è¶³åœ–å½¢ç¨ç«‹çµæ§‹çš„ æœ€å¤§æ¦‚ä¼¼ä¼°è¨ˆï¼ˆMLEï¼‰åˆ†å¸ƒã€‚

2. IPF å…¬å¼  
    å‡è¨­ç›®å‰æˆ‘å€‘åœ¨èª¿æ•´çš„ clique æ˜¯ (C)ï¼ˆä¾‹å¦‚ (C = {A, B})ï¼‰ã€‚

    å‰‡ IPF çš„æ›´æ–°é€šå¼æ˜¯ï¼š

    $$
    \hat{n}^{(t+1)}(x)
    =

    \hat{n}^{(t)}(x)
    \times
    \frac{n_{\text{obs}}(x_C)}{\hat{n}^{(t)}(x_C)}
    $$

    | ç¬¦è™Ÿ                                               | æ„ç¾©                                                |
    | ------------------------------------------------ | ------------------------------------------------- |
    | $x$                                              | ä¸€å€‹å®Œæ•´çš„çµ„åˆ   |
    | $\hat{n}^{(t)}(x)$                               | ç¬¬ (t) æ¬¡è¿­ä»£æ™‚ï¼Œè©²çµ„åˆçš„ fitted count                      |
    | $C$                                              | ç›®å‰è¦èª¿æ•´çš„ cliqueï¼ˆä¾‹å¦‚ {Gender, Eye}ï¼‰                   |
    | $x_C$                                            | è©²çµ„åˆä¸­å±¬æ–¼ clique C çš„éƒ¨åˆ†ï¼ˆä¾‹å¦‚ (Gender=female, Eye=blue)ï¼‰ |
    | $n_{\text{obs}}(x_C)$                            | åœ¨è§€å¯Ÿè³‡æ–™ä¸­ï¼Œclique C çš„é‚Šéš›è¨ˆæ•¸ï¼ˆå³é€™å€‹çµ„åˆçš„ç¸½æ•¸ï¼‰                   |
    | $\hat{n}^{(t)}(x_C)$                             | ç›®å‰ fitted counts çš„é‚Šéš›ï¼ˆå¾æ‰€æœ‰ cell åŠ ç¸½å¾—åˆ°ï¼‰               |
    | $\frac{n_{\text{obs}}(x_C)}{\hat{n}^{(t)}(x_C)}$ | èª¿æ•´æ¯”ä¾‹ï¼ˆè®“æ–°çš„ fitted margin åŒ¹é…è§€å¯Ÿåˆ°çš„ marginï¼‰             |









### (OK) (40A40B P.58~59)How to derive the formula of the **deviance of the fitted model**? 
å°ä¸€å€‹æ¨¡å‹ ( M )ï¼Œå…¶å°æ•¸ä¼¼ç„¶ç‚ºï¼š
$$
L_M = \sum_x n(x) \log \hat{P}_M(x)
$$

è€Œå°æ–¼ **é£½å’Œæ¨¡å‹ (saturated model)**ï¼ˆå³æ²’æœ‰ä»»ä½•é™åˆ¶çš„æ¨¡å‹ï¼‰ï¼š
$$
\hat{P}*{sat}(x) = \frac{n(x)}{N}
$$
å› æ­¤å®ƒçš„å°æ•¸ä¼¼ç„¶ç‚ºï¼š
$$
L*{sat} = \sum_x n(x) \log \frac{n(x)}{N}
$$

æ¨¡å‹ ( M ) çš„ deviance å®šç¾©ç‚ºï¼š
$$
\text{dev}(M) = 2 \times (L_{sat} - L_M)
$$

ä»£å…¥ä¸Šé¢å…©å€‹å°æ•¸ä¼¼ç„¶ï¼š
$$
\text{dev}(M)
= 2 \sum_x \Big[ n(x) \log \frac{n(x)}{N} - n(x) \log \hat{P}_M(x) \Big]
$$

$$
\text{dev}(M)
= 2 \sum_x n(x) \log \frac{n(x)}{\hat{n}(x)}
$$


$$
\boxed{
\text{dev}(M) = 2 \sum_{x} n(x) \log \frac{n(x)}{\hat{n}(x)}
}
$$

| ç¬¦è™Ÿ                              | æ„ç¾©                        |
| ------------------------------- | ------------------------- |
| $n(x)$| è§€å¯Ÿåˆ°çš„æ¬¡æ•¸ï¼ˆobserved countï¼‰    |
| $\hat{n}(x) = N \hat{P}_M(x)$| æ¨¡å‹é æ¸¬çš„æ¬¡æ•¸ï¼ˆfitted countï¼‰     |
| $N = \sum_x n(x)$| ç¸½æ¨£æœ¬æ•¸                      |
| $L_M$| æ¨¡å‹ ( M ) çš„ log-likelihood |
| $L_{sat}$| é£½å’Œæ¨¡å‹çš„ log-likelihood      |
| $\text{dev}(M)$| æ¨¡å‹åå·®ï¼ˆè¡¡é‡æ¨¡å‹èˆ‡è³‡æ–™çš„å·®è·ï¼‰          |

> **â€œDeviance = 2 Ã— Î£ (observed Ã— log(observed / fitted))â€**

ä¹Ÿå°±æ˜¯ï¼š

$$
\boxed{
\text{dev}(M) = 2 \sum_{\text{cells}} \text{observed} \times \log \frac{\text{observed}}{\text{fitted}}
}
$$

åƒè€ƒ exc 3 - problem 5 - c

### (OK) (40A40B P.60) **Deivance difference**
å‡è¨­ï¼š
$$
M_0 \subseteq M_1
$$
ä¹Ÿå°±æ˜¯èªªï¼š

> ( M_0 ) æ˜¯ **è¼ƒç°¡å–®çš„æ¨¡å‹**ï¼ˆå¤šä¸€äº›é™åˆ¶ï¼‰ï¼Œ  
> ( M_1 ) æ˜¯ **è¼ƒè¤‡é›œçš„æ¨¡å‹**ï¼ˆè‡ªç”±åº¦æ›´å¤šï¼‰ã€‚  
> $M_0$, $M_1$ æ˜¯åŸºæ–¼åŒä¸€å€‹è§€å¯Ÿè³‡æ–™ï¼Œåªæ˜¯ç”¨ä¸åŒçš„çµæ§‹æ“¬å’Œã€‚

$$
\boxed{
\text{Deviance difference} = \text{dev}(M_0) - \text{dev}(M_1)
}
$$

ç”±æ–¼ï¼š
$$
\text{dev}(M) = 2(L_{\text{sat}} - L_M)
$$

æ‰€ä»¥ï¼š
$$
\text{dev}(M_0) - \text{dev}(M_1)
= 2(L_{M_1} - L_{M_0})
$$


- **æœ€å¸¸è¦‹çš„è¡¨é”å¼**

$$
\boxed{
\Delta \text{dev} = 2 (L_{M_1} - L_{M_0})
}
$$

- **çµ±è¨ˆæª¢å®šæ„ç¾©**

ç•¶æ¨£æœ¬æ•¸ ( N ) å¾ˆå¤§æ™‚ï¼ˆå¤§æ¨£æœ¬è¿‘ä¼¼ï¼‰ï¼š

$$
2 (L_{M_1} - L_{M_0}) ;\approx; \chi^2_{\nu}
$$

å…¶ä¸­ï¼š

| ç¬¦è™Ÿ | æ„ç¾© |
| --- | --- |
| $\chi^2_{\nu}$ | å¡æ–¹åˆ†å¸ƒ (chi-square distribution) |
| $\nu$ | $M_0$ èˆ‡ $M_1$ ä¹‹é–“çš„è‡ªç”±åº¦å·®ï¼ˆæˆ–ã€Œé™åˆ¶æ•¸ã€ï¼‰ |
| $\nu$ = number of additional restrictions in $M_0$ compared to $M_1$ |


> **Deviance difference = 2 Ã— (log-likelihood gain of larger model)**
>
> ç”¨ä¾†æª¢é©—ï¼šå¢åŠ åƒæ•¸å¾Œï¼ˆå¾ $M_0$ â†’ $M_1$ï¼‰æ˜¯å¦é¡¯è‘—æå‡æ¨¡å‹æ“¬åˆã€‚

$$
\boxed{
\text{dev}(M_0) - \text{dev}(M_1) = 2(L_{M_1} - L_{M_0}) \sim \chi^2_{\nu}
}
$$


### (OK) Factorisation for conditional indepent model  
exc 3 - question 3 - b

In general, X is independent of Y given Z if and only if
$$
P(x,y|z) = P(x|z)P(y|z)
$$
for all values xof X, y of Y, and for all values z of Z with P(Z= z) >0 (otherwise the conditional probability is not defined). 

1. P(X,Y |Z) = P(X |Z)P(Y |Z)
2. P(X |Y,Z) = P(X |Z)

### (to study) Observed deviance > critical value æ™‚è¦ reject modelï¼Œç‚ºä»€éº¼ï¼Ÿ  
exc 3 - question 2 - d

1. ä»€éº¼æ˜¯ **observed deviance**ï¼Ÿ  
æ¨¡å‹çš„ deviance $\text{dev}(M)=2(L_{\text{sat}}-L_M)$ æ¸¬é‡ã€Œé£½å’Œæ¨¡å‹ï¼ˆå®Œç¾æ“¬åˆï¼‰èˆ‡ä½ è¦æª¢é©—çš„æ¨¡å‹ (M) ä¹‹é–“çš„å·®è·ã€ã€‚æ•¸å€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹ (M) ç„¡æ³•å¾ˆå¥½åœ°é‡ç¾è§€å¯Ÿåˆ°çš„è³‡æ–™ï¼ˆobserved vs fitted å·®è·å¤§ï¼‰ã€‚

2. ç‚ºä»€éº¼æŠŠ deviance ç•¶æˆæª¢å®šçµ±è¨ˆé‡ï¼Ÿ  
    ç•¶ $M_0$ ç‚ºã€Œè™›ç„¡å‡è¨­ï¼ˆnull hypothesisï¼‰ã€çš„æ¨¡å‹ï¼Œä¸” $M_1$ ç‚ºåŒ…å« $M_0$ çš„è¼ƒè¤‡é›œæ¨¡å‹ï¼ˆå·¢ç‹€æ¨¡å‹ï¼‰ï¼Œä¾æ“š **ä¼¼ç„¶æ¯”æª¢å®š(likelihood-ratio test)** èˆ‡ **Wilks' å®šç†**ï¼Œåœ¨æ¨£æœ¬æ•¸å¾ˆå¤§æ™‚
    $$
    2(L_{M_1}-L_{M_0})\quad\text{ï¼ˆä¹Ÿå°±æ˜¯};\text{dev}(M_0)-\text{dev}(M_1)\text{ï¼‰}
    $$
    è¿‘ä¼¼æœå¾å¡æ–¹åˆ†é…$(\chi^2_\nu)$ï¼Œ$\nu$ ç‚ºå…©æ¨¡å‹ä¹‹é–“åƒæ•¸æ•¸ç›®çš„å·®ï¼ˆæˆ–ç›¸ç•¶æ–¼ç´„æŸæ•¸ï¼‰ã€‚å› æ­¤å¯ä»¥ç”¨å¡æ–¹åˆ†å¸ƒçš„è‡¨ç•Œå€¼æˆ– p-value åšæª¢å®šã€‚

3. æ±ºç­–è¦å‰‡ï¼ˆç›´è§€ï¼‰
* è¨­é¡¯è‘—æ°´æº– $\alpha$ï¼ˆä¾‹å¦‚ 0.05ï¼‰ã€‚
* æ‰¾å‡ºå¡æ–¹åˆ†å¸ƒ $\chi^2_\nu$ åœ¨ $\alpha$ ä¸‹çš„è‡¨ç•Œå€¼ï¼ˆcritical valueï¼‰ã€‚
* è‹¥ **è§€å¯Ÿåˆ°çš„ deviance > è‡¨ç•Œå€¼**ï¼Œè¡¨ç¤ºåœ¨ $M_0$ ç‚ºçœŸæ™‚å‡ºç¾é€™éº¼å¤§çš„ deviance çš„æ©Ÿç‡ $<\alpha$ã€‚ä¹Ÿå°±æ˜¯èªªï¼šè³‡æ–™ **å¤ªä¸åƒ** åœ¨ $M_0$ ä¸‹æœƒå‡ºç¾çš„æƒ…å½¢ â†’ **æ‹’çµ• $M_0$**ã€‚æ›å¥è©±èªªï¼šå¤§ deviance â†’ è³‡æ–™èˆ‡ $M_0$ å·®ç•°é¡¯è‘— â†’ æ‹’çµ• $M_0$ã€‚

### (OK) (40A40B P.70,71) Hill-Climbing search
1. pick an initial model, e.g. the empty graph
2. neighbors
    - add an edge
    - delete an edge
    (without creating a chordless cycle of length > 3)
3. if all neighbors have higher AIC, stop and return the current model.
4. otherwise move to the neighbor with lowest AIC and return to 2.


## (OK) 41A 41B Frequent pattern mining for **sets**, **sequences** and **trees**
### (OK) (41A P.8~11) What is **Apriori algorithm**, what is the meaning?
Apriori algorithm å¯ä»¥æ‰¾å‡ºæ‰€æœ‰ frequent itemsetsã€‚

1. Apriori algorithm çš„å¯¦éš›æ“ä½œ
    çµ¦å®šå¥½å¹¾ç­†ä¸åŒçš„transactionsï¼Œæ¯ä¸€ç­†transcationséƒ½æ˜¯æœ‰ä¸åŒçš„å€¼(A, B, C, D, E)çµ„æˆçš„setï¼Œä¸¦ä¸”å†çµ¦å®šä¸€å€‹minimun supportã€‚

    å¾ä¸€å€‹setè£¡é¢åªæœ‰ä¸€ç­†è³‡æ–™é–‹å§‹ï¼Œæ‰¾æ¯å€‹setåŒ…å«åœ¨å¹¾çš„transactionsè£¡é¢ï¼Œé€™é€™å°±æ˜¯ä»–çš„supportï¼Œå¦‚æœé€™å€‹support > minimun supportï¼Œå°±å¸¶è¡¨ä»–æ˜¯frequentã€‚æ¥è‘—ï¼Œå†æŠŠæ‰€æœ‰frequentçš„setsåšçµ„åˆï¼Œè®Šæˆä¸€å€‹setè£¡é¢æœ‰å…©ç­†è³‡æ–™ï¼Œä»¥æ­¤é¡æ¨ã€‚

    åœ¨æ‰¾ä¸‹å€‹levelçš„candiateæ™‚ï¼Œè¦æ³¨æ„ï¼Œå¦‚æœçµ„åˆå‡ºçš„setï¼Œä»–çš„subsetæœ‰å·²ç¶“ä¸æ˜¯frequentçš„setï¼Œå°±ç›´æ¥åˆªé™¤ï¼Œä¸ä½œç‚ºä¸‹ä¸€å€‹levelçš„candidateã€‚

    summerize: ç•«å‡ºå„å€‹levelçš„ â€œcandidate, support, frequent?â€ è¡¨æ ¼ã€‚

2. Aprioir algorithm æ˜¯ä»€éº¼ï¼Ÿ  
    ä¸»è¦ç”¨æ–¼å¾å¤§å‹è³‡æ–™é›†ä¸­æ‰¾å‡ºé »ç¹é …é›†(frequent itemsets)ï¼Œé€²è€Œç”¢ç”Ÿæœ‰æ„ç¾©çš„é—œè¯è¦å‰‡(association rules)ã€‚ï¼ˆex:{ç‰›å¥¶} â†’ {éºµåŒ…} è²·ç‰›å¥¶çš„äººå¸¸å¸¸ä¹Ÿæœƒè²·éºµåŒ…)

    Apriori property:
        - å¦‚æœä¸€å€‹itemsetsæ˜¯é »ç¹çš„ï¼Œé‚£ä»–æ‰€æœ‰subsetsä¹Ÿä¸€å®šæ˜¯é »ç¹çš„ã€‚
        - å¦‚æœä¸€å€‹itemsetsæ˜¯ä¸é »ç¹çš„ï¼Œé‚£ä»–æ‰€æœ‰supersetsä¹Ÿä¸€å®šæ˜¯ä¸é »ç¹çš„ã€‚


### (OK) What is the meaning of **closed frequent itemsets**?
Closed Frequent Itemsets çš„å®šç¾©æ›å¥è©±èªªï¼š
    1. é€™å€‹setæ˜¯frequentã€‚
    2. å†åŠ ä¸Šå…¶ä»–é …ç›®é€²å»ï¼Œå‡ºç¾çš„æ¬¡è¼¸å°±æœƒè®Šå°‘ã€‚

ä¹Ÿå°±æ˜¯åªä¿ç•™çœŸæ­£ä»£è¡¨è³‡æ–™åˆ†å¸ƒï¼Œè€Œä¸æ˜¯è¢«æ›´å¤§é›†åˆé‡è¤‡è¦†è“‹çš„setsã€‚

### (OK) (41A P.25) What is the meaning of **maximal frequent itemset**?
### (OK) (41A P.26,40~41) What is Apriori-close algorithm, what is the meaning?
Apriori-Close algorithm å¯ä»¥æ‰¾å‡ºæ‰€æœ‰ closed frequent itemsetsã€‚

1. å¦‚ä½•å¯¦ä½œaprioir-close algorithm?  
    åŸºæœ¬ä¸Šæ“ä½œçš„æ–¹æ³•è·Ÿapriori algorithmå·®ä¸å¤šï¼Œä½†æ˜¯åœ¨pruneæ™‚ï¼Œé™¤äº†è€ƒæ…®support > minimum supportçš„æ¢ä»¶ä»¥å¤–ï¼Œé‚„è¦é¡å¤–è€ƒæ…®é€™å€‹setçš„supportæ˜¯å¦åˆ†ä»–çš„subsetä¸€æ¨£ï¼Œå¦‚æœä¸€æ¨£çš„è©±ï¼Œé€™å€‹setå°±è¦è¢«å‰ªæ‰ã€‚
    
    é€™æ¨£å°±æ‰¾åˆ°ä»–æ‰€æœ‰çš„generatorï¼Œæ¥è‘—å†é€éé€™äº›generatoræ‰¾åˆ°ä»–å€‘çš„closureã€‚

    æ‰¾colsureçš„æ–¹æ³•ï¼šæ‰¾åˆ°æ‰€æœ‰åŒ…å«æŸå€‹generatorçš„transactionsï¼Œæ¥è‘—æ‰¾é€™äº›transactionsçš„æœ€å¤§äº¤é›†ã€‚

    summerize: ç¬¬ä¸€æ¬¡ç•«å‡º â€œcandidate, support, generator?â€ å¾ç¬¬ä¸€å€‹levelé–‹å§‹çš„å„å€‹è¡¨æ ¼ã€‚æ¥è‘—é€éæ‰€æœ‰çš„generatoræ‰¾closureå’Œsupportï¼Œä¹Ÿå°±æ˜¯ç•«ä¸€å€‹â€œgenerator, closure, supportâ€çš„è¡¨æ ¼ã€‚
    
2. Apriori-close algorithm æ˜¯ä»€éº¼ï¼Ÿ
    Apriori-Close æ˜¯ä¸€ç¨®å¾äº¤æ˜“è³‡æ–™ä¸­æ‰¾å‡ºé–‰åˆé »ç¹é …é›†(Closed Frequent Itemsets)çš„æ¼”ç®—æ³•ã€‚
An itemset I is maximal frequent if
    - I is frequent and
    - no proper superset of I is frequent

å¯ä»¥å…ˆç”¨ Apriori algorithm æ‰¾å‡ºæ‰€æœ‰ frequent itemsetsï¼Œæˆ–è€…ç”¨ Apriori-Close algorithm æ‰¾å‡ºæ‰€æœ‰ closed frequent itemsetsï¼Œç„¶å¾Œå†å¾ä¸­æŒ‘å‡ºé‚£äº›æ²’æœ‰ä»»ä½•é »ç¹è¶…é›†çš„é …é›†ï¼Œå°±èƒ½å¾—åˆ° maximal frequent itemsetsã€‚


    Closed Frequent Itemsets çš„å®šç¾©ï¼šä¸€å€‹setæ˜¯closedï¼Œå¦‚æœæ²’æœ‰ä»»ä½•ä»–çš„supersetæœ‰ç›¸åŒçš„supportã€‚


### (OK) (41A P.18,23) What is the meaning of **confidence** and **lift**?
åœ¨**é—œè¯è¦å‰‡ï¼ˆAssociation Rulesï¼‰**ä¸­ï¼Œ`confidence`ï¼ˆä¿¡è³´åº¦ï¼‰èˆ‡ `lift`ï¼ˆæå‡åº¦ï¼‰æ˜¯å…©å€‹éå¸¸é‡è¦çš„è©•ä¼°æŒ‡æ¨™ï¼Œç”¨ä¾†åˆ¤æ–·è¦å‰‡æ˜¯å¦æœ‰æ„ç¾©ã€‚

1. Confidenceï¼ˆä¿¡è³´åº¦ï¼‰
    å¦‚æœæˆ‘å€‘æœ‰ä¸€æ¢é—œè¯è¦å‰‡ï¼š
    $$
    A \Rightarrow B
    $$

    é‚£éº¼å®ƒçš„ **ä¿¡è³´åº¦ï¼ˆconfidenceï¼‰** å®šç¾©ç‚ºï¼š
    $$
    \text{confidence}(A \Rightarrow B) = \frac{\text{support}(A \cup B)}{\text{support}(A)}
    $$

    åœ¨æ‰€æœ‰åŒ…å« (A) çš„äº¤æ˜“ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹åŒæ™‚ä¹ŸåŒ…å« (B)ã€‚

    ğŸ‘‰ æ›å¥è©±èªªï¼Œä¿¡è³´åº¦ä»£è¡¨ï¼š

    > ã€Œç•¶é¡§å®¢è²·äº† A æ™‚ï¼Œä¹Ÿæœƒè²· B çš„æ©Ÿç‡ã€ã€‚

2. Liftï¼ˆæå‡åº¦ï¼‰
    $$
    \text{lift}(X \Rightarrow Y) = \frac{P(Y|X)}{P(Y)} = \frac{P(X \cap Y)}{P(X)P(Y)}
    $$

    è§£é‡‹ï¼š

    * $P(Y)$ï¼šåœ¨æ‰€æœ‰è³‡æ–™ä¸­ï¼ŒY å‡ºç¾çš„æ©Ÿç‡ã€‚
    * $P(Y|X)$ï¼šåœ¨å·²ç¶“çŸ¥é“ X å‡ºç¾çš„æ¢ä»¶ä¸‹ï¼ŒY å‡ºç¾çš„æ©Ÿç‡ã€‚
    * è‹¥ Xã€Y **ç¨ç«‹**ï¼Œç†è«–ä¸Š $P(Y|X) = P(Y)$ã€‚

    æ‰€ä»¥ï¼š

    * è‹¥ $\text{lift} = 1$ï¼šX èˆ‡ Y ç¨ç«‹ï¼ˆæ²’æœ‰é—œè¯ï¼‰ï¼›
    * è‹¥ $\text{lift} > 1$ï¼šX å‡ºç¾æœƒè®“ Y æ›´å¯èƒ½å‡ºç¾ â†’ **æ­£ç›¸é—œ**ï¼›
    * è‹¥ $\text{lift} < 1$ï¼šX å‡ºç¾æœƒè®“ Y è®Šå¾—ä¸å¤ªå¯èƒ½å‡ºç¾ â†’ **è² ç›¸é—œ**ã€‚


### (OK) (41B P.9~13) What is GSP algorithm?
GSP (Generalized Sequential Pattern algorithm) ç”¨ä¾†åœ¨è³‡æ–™ä¸­æ‰¾å‡ºç¶“å¸¸åœ¨ä¸€èµ·ï¼Œè€Œä¸”æœ‰åºåˆ—é—œä¿‚çš„é …ç›®åºåˆ— (sequential patterns)ã€‚

ä¸€æ¨£å¾level 1 é–‹å§‹ç•«å‡ºæ‰€æœ‰levelsçš„ â€œcandidate, support, frequent?â€ è¡¨æ ¼ã€‚

åœ¨ä¸€å€‹levelçš„æ‰€æœ‰candidateæ‰¾å‡ºä¾†å¾Œï¼Œè¦å…ˆç¢ºèªä»–çš„support > minimun supportã€‚

åœ¨çµ„ä¸‹ä¸€å€‹levelçš„candidateæ™‚ï¼Œè¦çœ‹ç›®å‰sequenceé•·åº¦(n) - 1å€‹å€¼ï¼Œå…©å€‹sequencesçš„å‰ n-1 å€‹å€¼è¦ä¸€æ¨£ï¼Œæ¥è‘—å†æŠŠæœ€å¾Œä¸€å€‹å€¼çš„æ‰€æœ‰å¯èƒ½åˆä½µã€‚åˆä½µæ™‚ä¸€æ¨£è¦è€ƒæ…®ä¸è¦åˆä½µå‡ºsubsetå·²ç¶“ä¸æ˜¯frequentçš„å€¼ã€‚

### (OK) (41B P.24,27) Induced subtree V.S. embedded subtree
- Induced subtree  
ä¸å…è¨±è·³éä¸­é–“çš„ç¯€é»ï¼Œä¸€å®šè¦ï¼Œä¸€å®šè¦ä¿æŒçˆ¶å­é—œä¿‚ã€‚

    1. æ¨™ç±¤ç›¸åŒ(label preservation)
    2. ä¿æŒå·¦å³è¨“çºŒ(orser preservation)
    3. ä¿æŒçˆ¶å­é—œä¿‚(parent-child preservation)

- Embadded subtree  
å¯ä»¥è·³éä¸­é–“çš„ç¯€é»ï¼Œåªè¦ç¥–å…ˆèˆ‡å¾Œä»£çš„é—œä¿‚ä¸è®Šï¼Œå°±ç®—æ˜¯åŒ¹é…ã€‚
    1. æ¨™ç±¤ç›¸åŒ(label preservation)
    2. ä¿æŒå·¦å³è¨“çºŒ(orser preservation)
    3. ä¿æŒç¥–å…ˆ-å¾Œä»£é—œä¿‚(ancestor-descendant preservation)

### (OK) (41B P.25) What is maching function?
å‡è¨­æˆ‘å€‘æœ‰å…©æ£µæ¨¹ï¼š

* **T**ï¼špattern treeï¼ˆæ¨¡å¼æ¨¹ï¼Œé€šå¸¸æ¯”è¼ƒå°ï¼‰
* **D**ï¼šdata treeï¼ˆè³‡æ–™æ¨¹ï¼Œé€šå¸¸æ¯”è¼ƒå¤§ï¼‰

matching function è¨˜ä½œï¼š

$$
\phi : V_T \to V_D
$$
> å°‡æ¨¡å¼æ¨¹ T ä¸­çš„æ¯å€‹ç¯€é» ( w_i ) **å°æ‡‰**ï¼ˆmapï¼‰åˆ°è³‡æ–™æ¨¹ D ä¸­çš„ä¸€å€‹ç¯€é» ( v_j )ã€‚

æœ€å¾Œæœƒé•·å¾—åƒé€™æ¨£
Ï†(w1) = v1 Ï†(w2) = v2 Ï†(w3) = v3
w1, w2, w3 æ˜¯T(pattern tree)çš„ç¯€é»ï¼Œv1, v2, v3 æ˜¯å°æ‡‰åˆ°çš„ D(data tree)çš„ç¯€é»ã€‚

### (OK) (41B) What is FREQT, right-most occorence list(RMO list)?
1. ä»€éº¼æ˜¯FREQT(Frequent Tree Mining algorighm)  
ç›®æ¨™å¾å¤šå€‹data treesä¸­æ‰¾å‡ºæ‰€æœ‰å¹³å‡¡å‡ºç¾çš„frequent subtreesã€‚  

å°±åƒæ˜¯ Poriori æ‰¾ frequent itemsetsæˆ–æ˜¯ GSPæ‰¾frequent sequenceä¸€æ¨£ã€‚  

FREQTè¦åšçš„å…©ä»¶äº‹ï¼š
    1. ç”Ÿæˆå€™é¸æ¨¹ (generate candidate trees)
    2. å¹¾ç®—æ¯é¡†å€™é¸æ•¸å‡ºç¾çš„æ¬¡æ•¸

2. Right-Most Extension  
é€™æ˜¯FREQTç”Ÿæˆå€™é¸æ•¸çš„æ–¹å¼ã€‚  

åœ¨å°Tåšdepth-first preorder traversalå¾Œï¼Œç¯€é»ä¸€é †åºç·¨è™Ÿï¼Œæ‰¾å‡ºå¾è·Ÿç¯€é»åˆ°æœ€å¾Œä¸€å€‹ç¯€é»çš„è·¯å¾‘ï¼ˆå³ç«¯è·¯å¾‘ï¼‰ï¼Œæ–°çš„ç¯€é»åªèƒ½åŠ åœ¨é€™æ¢è·¯å¾‘ä¸Šçš„ç¯€é»ä¹‹ä¸€çš„å³ç«¯ä½ç½®ã€‚

3. <mark>Right-Most Occurence List(RMO list)</mark>  
RMO æœƒè¨˜éŒ„é€™å€‹ pattern tree çš„ right-most leafåœ¨æ¯ä¸€é¡†data treeä¸­å‡ºç¾çš„æ‰€æœ‰ä½ç½®ã€‚ï¼ˆä¸€å€‹pattern tree å¯èƒ½æœƒå‡ºç¾åœ¨ ä¸€å€‹data tree å¥½å¹¾æ¬¡ï¼Œæœƒç´€éŒ„å‡ºç¾çš„æ¯ä¸€æ¬¡çš„ritht-most leafï¼‰  

åªç´€éŒ„æ¯ä¸€å€‹matchçš„right most nodeï¼Œä½†æ˜¯æœƒé–“æ¥åŒ…å«æ•´æ¢right most pathçš„è³‡è¨Šã€‚å› ç‚ºé€éright most nodeçš„ä½ç½®å°±å¯ä»¥è¿½æº¯å‡ºright most path ä¸Šæ‰€æœ‰ç¯€é»åœ¨data tree ä¸­çš„å°æ‡‰ä½ç½®ã€‚

ç›®çš„æ˜¯ç‚ºäº†è®“ä¸‹ä¸€æ¬¡çš„æœå°‹è®Šå¾—æ›´ç°¡å–®ï¼Œåªè¦å¾æœ‰è¢«è¨˜éŒ„çš„é‚£å¹¾å€‹ä½ç½®å¾€ä¸‹æœå°‹å°±å¯ä»¥äº†ã€‚

RMO list æ˜¯ç‚ºäº†induced subtree miningè¨­è¨ˆçš„ï¼Œä¸é©ç”¨æ–¼embedded subtreesã€‚

è«‹åƒè€ƒ exc4 - question3 - (h)

## (OK) 42A 42B Bayesian Networks
### (OK) What is **Bayesian Network** and **DAG** (Directed Acyclic Graph)
DAG (Directed Acyclic Graph) æ˜¯ æœ‰å‘ç„¡ç’°åœ–ã€‚  
`Bayesian Networkï¼ˆè²è‘‰æ–¯ç¶²è·¯ï¼‰ = ä¸€å€‹å¸¶æœ‰æ©Ÿç‡æ„ç¾©çš„æœ‰å‘ç„¡ç’°åœ–ï¼ˆDAGï¼‰`  
ä¹Ÿå°±æ˜¯èªªï¼ŒDAG æ˜¯ä¸€å€‹çµæ§‹ï¼ˆStructureï¼‰ï¼Œæè¿°å“ªäº›è®Šé‡ä¹‹é–“æœ‰ã€Œä¾è³´é—œä¿‚ã€ã€‚  
Bayesian Network å‰‡æ˜¯åœ¨é€™å€‹çµæ§‹ä¸Šï¼ŒåŠ å…¥äº†æ©Ÿç‡åˆ†å¸ƒï¼ˆParametersï¼‰ çš„æ¨¡å‹ã€‚

### (OK) (42A P.14~22) **Construction of DAG** and **Factorization**
åœ¨construction of DAGçš„éç¨‹ä¸­ï¼Œå°±å¯ä»¥çœ‹å‡ºDAGèˆ‡ä»–çš„faxtorizationä¹‹é–“çš„é—œä¿‚ã€‚
1. <mark>Construction of DAG</mark>
æ ¹æ“šè®Šé‡ä¹‹é–“çš„æ¢ä»¶ç¨ç«‹é—œä¿‚ï¼Œæ±ºå®šæ¯å€‹ç¯€é»çš„çˆ¶ç¯€é»ï¼Œä¸¦ç•«å‡ºå°æ‡‰çš„æœ‰å‘é‚Šã€‚
    1. ä¸€é–‹å§‹å…ˆå‡è¨­å®Œå…¨æœ‰ç›¸åœ–
    2. åˆ©ç”¨æ¢ä»¶ç¨ç«‹æ€§åˆªé‚Šï¼Œä¸¦ä¸”ä¹ŸåŒæ™‚å°P(X)åšèª¿æ•´ã€‚
    3. æœ€å¾Œç”¢ç”Ÿä¸€å€‹DAGï¼Œä»¥åŠä¸€å€‹P(X)å…¬å¼ã€‚è€Œé€™å€‹P(X)å…¬å¼å°±æ˜¯é€™å€‹DAGçš„factorizationã€‚
2. DAG çš„ Factorizationæ€éº¼å¾—å‡ºï¼Ÿ  
    å°æ–¼ä¸€å€‹æœ‰ $k$ å€‹éš¨æ©Ÿè®Šæ•¸çš„ç³»çµ±
    $$
    X = (X_1, X_2, \dots, X_k)
    $$
    ä»¥åŠå°æ‡‰çš„ **æœ‰å‘ç„¡ç’°åœ– (DAG)** çµæ§‹ $G$ï¼Œ
    å…¶å°æ‡‰çš„æ©Ÿç‡åˆ†è§£ï¼ˆfactorizationï¼‰ç‚ºï¼š

    $$
    P(X_1, X_2, \dots, X_k) = \prod_{i=1}^{k} P(X_i \mid \text{pa}(X_i))
    $$

    |              ç¬¦è™Ÿ              | æ„æ€                                         |
    | :--------------------------: | :----------------------------------------- |
    |   $P(X_1, X_2, \dots, X_k)$  | è¯åˆæ©Ÿç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰è®Šæ•¸åŒæ™‚å‡ºç¾çš„æ©Ÿç‡ï¼‰                        |
    |             $X_i$            | ç¬¬ $i$ å€‹éš¨æ©Ÿè®Šæ•¸                                |
    |       $\text{pa}(X_i)$       | $X_i$ çš„çˆ¶ç¯€é»ï¼ˆparentsï¼‰ï¼Œå³åœ¨ DAG ä¸­æŒ‡å‘ $X_i$ çš„ç¯€é»é›†åˆ |
    | $P(X_i \mid \text{pa}(X_i))$ | åœ¨çµ¦å®šå…¶çˆ¶ç¯€é»å€¼æ™‚ï¼Œ$X_i$ çš„æ¢ä»¶æ©Ÿç‡                      |
    |       $\prod_{i=1}^{k}$      | è¡¨ç¤ºå°æ‰€æœ‰ç¯€é»çš„ä¹˜ç©ï¼ˆå› ç‚ºæ¯å€‹ç¯€é»å°æ‡‰ä¸€å€‹æ¢ä»¶æ©Ÿç‡é …ï¼‰                |

    çµè«–ï¼šçµ¦å®šä¸€å€‹ DAGï¼Œå°±å¯ä»¥å”¯ä¸€åœ°å¯«å‡ºå…¶ factorization å…¬å¼ã€‚åä¹‹ï¼Œè‹¥çµ¦å®š factorization å½¢å¼ï¼Œä¹Ÿèƒ½æ¨å°å‡ºå°æ‡‰çš„ DAGã€‚  

    ç°¡å–®ä¾†èªªï¼Œæ¯å€‹nodeä»¥ä»–çš„parent(s)ç‚ºæ¢ä»¶çš„æ©Ÿç‡ ç›¸ä¹˜ï¼Œå°±æ˜¯ä¸€å€‹DAGçš„factorizationã€‚

### (OK) (42A P.24~33) Moral graph
1. What is moral graph?  
çµ¦å®šä¸€å€‹DAGï¼Œä»–å°æ‡‰çš„moral graph æ˜¯ä¸€å€‹ç„¡ç›¸åœ–ï¼Œä¸¦ä¸”è¦æ ¹æ“šä»¥ä¸‹çš„è¦å‰‡å»ºæ§‹è©²ç„¡ç›¸åœ–ï¼š
    1. Marry the parents: å°æ–¼æ¯å€‹nodeï¼ŒæŠŠä»–çš„æ‰€æœ‰parentså…©å…©ç›¸é€£ã€‚  
    2. Drop the directions: å°‡æ‰€æœ‰çš„æœ‰å‘é‚Šè®Šæˆç„¡ç›¸é‚Šã€‚

2. å¦‚ä½•ä½¿ç”¨ moral graph åœ¨ Bayesian Network çš„é€²è¡Œæ¢ä»¶ç¨ç«‹æ€§åˆ†æï¼Ÿ  
åœ¨bayesian network(DAG)ä¸­ï¼Œè¦åˆ¤æ–·å…©å€‹nodesæ˜¯å¦ç¨ç«‹ï¼Œç†è«–ä¸Šæ‡‰è©²ä½¿ç”¨d-separationï¼ˆæ–¹å‘æ€§åˆ†é›¢ï¼‰è¦å‰‡ï¼Œä½†æ˜¯é€™å€‹è¦å‰‡ç›¸å°è¤‡é›œã€‚è€Œå¦ä¸€å€‹æ–¹æ³•æ˜¯æŠŠDAGè½‰æˆMoral Graphï¼Œç„¶å¾Œç”¨æ›´ç°¡å–®çš„graph separationï¼ˆæ™®é€šåœ–åˆ†é›¢ï¼‰ä¾†åˆ¤æ–·ç¨ç«‹æ€§ã€‚
    1. å…ˆç¢ºå®šæƒ³çŸ¥é“çš„ç¨ç«‹æ€§é—œä¿‚æœ‰å“ªäº›è®Šé‡(nodes)
    2. åªè€ƒæ…®é€™äº›nodesä»¥åŠä»–å€‘çš„æ‰€æœ‰parents
    3. Moralize the DAG
    4. é€²è¡Œç°¡å–®çš„graph separation ä»¥æª¢æŸ¥ç¨ç«‹æ€§ï¼šåˆªé™¤conditioned nodesä»¥åŠé€£æ¥ä»–çš„é‚Šï¼Œæª¢æŸ¥æ¬²åˆ¤æ–·çš„nodesæœ‰æ²’æœ‰åˆ†é›¢ã€‚

### ~~Why moral graph can check Independence Properties?~~

### (OK) (42B P.8) Scoring Functions AIC & BIC
1. å¦‚ä½•è¨ˆç®—é€™å…©å€‹å€¼ï¼Œæ•¸å­¸ç¬¦è™Ÿæ˜¯ä»€éº¼  
**AIC**ï¼ˆAkaike Information Criterionï¼‰ å’Œ **BIC**ï¼ˆBayesian Information Criterionï¼‰æ˜¯ç”¨åœ¨ **è²è‘‰æ–¯ç¶²è·¯çµæ§‹å­¸ç¿’ï¼ˆStructure Learningï¼‰** çš„ **æ¨¡å‹è©•åˆ†å‡½æ•¸ (Scoring Functions)**ã€‚

    1. **AICï¼ˆAkaike Information Criterionï¼‰**
        $$
        \text{AIC}(M) = L_M - \text{dim}(M)
        $$
        æˆ–æ›´å¸¸è¦‹çš„çµ±è¨ˆå¯«æ³•ï¼ˆå¸¶å¸¸æ•¸ï¼‰ï¼š
        $$
        \text{AIC}(M) = 2L_M - 2 \text{dim}(M)
        $$
        åœ¨è²è‘‰æ–¯ç¶²è·¯ä¸­å¸¸ç°¡åŒ–æ‰å¸¸æ•¸ 2ï¼ˆå› ç‚ºæ¯”è¼ƒæ¨¡å‹æ™‚ä¸å½±éŸ¿æ’åï¼‰ï¼Œæ‰€ä»¥å¯ä»¥å¯«æˆä¸Šé¢é‚£æ¨£ã€‚
        |         ç¬¦è™Ÿ        | åç¨±             | å«ç¾©                                 |
        | :---------------: | :------------- | :--------------------------------- |
        |       ( M )       | æ¨¡å‹             | æŒ‡ä¸€å€‹è²è‘‰æ–¯ç¶²è·¯çµæ§‹                         |
        |      ( L_M )      | log-likelihood | çµ¦å®šæ¨¡å‹ ( M ) ä¸‹çš„å°æ•¸ä¼¼ç„¶å€¼ï¼Œåæ˜ æ¨¡å‹å°æ•¸æ“šçš„æ“¬åˆç¨‹åº¦    |
        | ( \text{dim}(M) ) | æ¨¡å‹ç¶­åº¦           | æ¨¡å‹ä¸­è‡ªç”±åƒæ•¸çš„å€‹æ•¸ |
        | ( \text{AIC}(M) ) | AIC åˆ†æ•¸         | è¶Šå¤§ï¼ˆæˆ–è¶Šå°ï¼Œå–æ±ºæ–¼å®šç¾©ï¼‰è¡¨ç¤ºæ¨¡å‹è¶Šå¥½ï¼Œä½†è¦åœ¨æ“¬åˆèˆ‡ç°¡æ½”ä¹‹é–“å–å¾—å¹³è¡¡ |

    2. BICï¼ˆBayesian Information Criterionï¼‰

        $$
        \text{BIC}(M) = L_M - \frac{log n}{2} \text{dim}(M) \
        $$

        |                 ç¬¦è™Ÿ                 | åç¨±                | å«ç¾©               |
        | :--------------------------------: | :---------------- | :--------------- |
        |                ( M )               | æ¨¡å‹                | ä¸€å€‹å€™é¸çš„è²è‘‰æ–¯ç¶²è·¯çµæ§‹     |
        |               ( L_M )              | log-likelihood    | çµ¦å®šæ¨¡å‹ ( M ) çš„å°æ•¸ä¼¼ç„¶ |
        |          ( \text{dim}(M) )         | æ¨¡å‹ç¶­åº¦              | æ¨¡å‹çš„åƒæ•¸æ•¸é‡          |
        |                ( n )               | æ¨£æœ¬æ•¸               | ç”¨ä¾†ä¼°è¨ˆæ¨¡å‹çš„è³‡æ–™é»æ•¸é‡     |
        | ( \frac{1}{2}\text{dim}(M)\log n ) | æ‡²ç½°é …ï¼ˆpenalty termï¼‰ | è³‡æ–™è¶Šå¤šã€åƒæ•¸è¶Šå¤šï¼Œæ‡²ç½°è¶Šé‡   |
        |          ( \text{BIC}(M) )         | BIC åˆ†æ•¸            | è¡¡é‡æ¨¡å‹çš„æ“¬åˆèˆ‡è¤‡é›œåº¦æŠ˜è¡·    |

2. BIC Score è¶Šé«˜è¶Šå¥½ï¼Œç‚ºä»€éº¼ï¼Ÿexc5-problem3-e-solè£¡é¢æœ‰è¬›åˆ°ã€‚

    1. log-likelihood $L_M$  
    $L_M$ è¶Šå¤§ â†’ æ¨¡å‹æ“¬åˆè³‡æ–™è¶Šå¥½ã€‚

    2. æ‡²ç½°é … $- \text{dim}(M)$ æˆ– $- \frac{log n}{2} \text{dim}(M)$  
    æ‡²ç½°è¶Šå¤§ â†’ æ¨¡å‹è¶Šè¤‡é›œã€è¦æ‰£åˆ†ã€‚

    3. AIC / BIC ç¸½å’Œ  
    $L_M - \text{penalty}$ï¼šæƒ³è¦ **æ“¬åˆé«˜ã€æ‡²ç½°ä½** â†’ å€¼è¶Šå¤§è¶Šå¥½ã€‚  
- é€™é æŠ•å½±ç‰‡æ¯ä¸€é çš„ç¬¦è™Ÿä»£è¡¨ä»€éº¼
- ç‚ºä»€éº¼ç¬¬å…­é å¯ä»¥ç°¡åŒ–æˆæœ€ä¸‹é¢çš„å…¬å¼


### (OK) (42B P.6,7) How to count maximum likelihood & log likelihood in scoring functions(AIC, BIC)?
Maximum likelihood çš„è¨ˆç®— exercise 5 - question 3 - (a) æœ‰ä¾‹å­å¯ä»¥åƒè€ƒã€‚  
log-likelihood çš„è¨ˆç®—exercise 5 - question 3 - (b) æœ‰ä¾‹å­å¯ä»¥åƒè€ƒã€‚  

1. **Maximum Likelihood Estimation**

    |                  ç¬¦è™Ÿ                  |       åç¨±       | å«ç¾©                                                              |
    | :----------------------------------: | :------------: | :-------------------------------------------------------------- |
    |                 $X_i$                |   ç¬¬ $i$ å€‹éš¨æ©Ÿè®Šé‡  | è²è‘‰æ–¯ç¶²è·¯ä¸­çš„ç¯€é»ä¹‹ä¸€                                                     |
    |            $\text{pa}(i)$            |      çˆ¶ç¯€é»é›†åˆ     | æ‰€æœ‰ç›´æ¥æŒ‡å‘ $X_i$ çš„è®Šé‡é›†åˆ                                              |
    |                 $x_i$                | è®Šé‡ $X_i$ çš„å…·é«”å–å€¼ | ä¾‹å¦‚ $X_i = 0$ æˆ– $X_i = 1$                                        |
    |          $x_{\text{pa}(i)}$          |    çˆ¶ç¯€é»çš„å–å€¼çµ„åˆ    | ä¾‹å¦‚è‹¥ $\text{pa}(i)={X_1, X_2}$ï¼Œå‰‡ $x_{\text{pa}(i)} = (x_1, x_2)$ |
    |      $n(x_i, x_{\text{pa}(i)})$      |     è¯åˆå‡ºç¾æ¬¡æ•¸     | è³‡æ–™ä¸­æ»¿è¶³ $X_i = x_i$ ä¸” $X_{\text{pa}(i)} = x_{\text{pa}(i)}$ çš„ç´€éŒ„æ•¸  |
    |         $n(x_{\text{pa}(i)})$        |    çˆ¶ç¯€é»çµ„åˆå‡ºç¾æ¬¡æ•¸   | è³‡æ–™ä¸­ $X_{\text{pa}(i)} = x_{\text{pa}(i)}$ çš„ç´€éŒ„æ•¸                  |
    | $\hat{p}(x_i \mid x_{\text{pa}(i)})$ |   æ¢ä»¶æ©Ÿç‡çš„æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ  | ä»¥é »ç‡ä¼°è¨ˆçš„æ¢ä»¶æ©Ÿç‡                                                      |

    ---

    è‹¥ $X_i$ **æ²’æœ‰çˆ¶ç¯€é»**ï¼ˆå³ $\text{pa}(i) = \varnothing$ï¼‰ï¼Œå‰‡ï¼š

    $$
    \hat{p}(x_i) = \frac{n(x_i)}{n}
    $$

    |    ç¬¦è™Ÿ    | å«ç¾©                   |
    | :------: | :------------------- |
    | $n(x_i)$ | è³‡æ–™ä¸­ $X_i = x_i$ çš„ç´€éŒ„æ•¸ |
    |    $n$   | è³‡æ–™çš„ç¸½ç´€éŒ„æ•¸              |

2. **Log-Likelihood Score**

    $$
    L = \sum_{i=1}^{k} \sum_{x_i,, x_{\text{pa}(i)}} n(x_i, x_{\text{pa}(i)}) \log \frac{n(x_i, x_{\text{pa}(i)})}{n(x_{\text{pa}(i)})}
    $$

    |                              ç¬¦è™Ÿ                             |              åç¨±              | å«ç¾©                 |
    | :---------------------------------------------------------: | :--------------------------: | :----------------- |
    |                             $L$                             | Log-likelihood scoreï¼ˆå°æ•¸ä¼¼ç„¶åˆ†æ•¸ï¼‰ | è¡¡é‡æ¨¡å‹å°è³‡æ–™çš„æ“¬åˆç¨‹åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ |
    |                             $k$                             |             ç¯€é»ç¸½æ•¸             | è²è‘‰æ–¯ç¶²è·¯ä¸­è®Šé‡çš„æ•¸é‡        |
    |               $\sum_{x_i,, x_{\text{pa}(i)}}$               |         å°æ‰€æœ‰å¯èƒ½çš„å–å€¼çµ„åˆæ±‚å’Œ         | æ¯å€‹ç¯€é»åŠå…¶çˆ¶ç¯€é»çš„æ‰€æœ‰å€¼çµ„åˆ    |
    |                  $n(x_i, x_{\text{pa}(i)})$                 |             è¯åˆæ¬¡æ•¸             | å¦‚ä¸Šæ‰€è¿°               |
    |                    $n(x_{\text{pa}(i)})$                    |            çˆ¶ç¯€é»å‡ºç¾æ¬¡æ•¸           | å¦‚ä¸Šæ‰€è¿°               |
    | $\log \frac{n(x_i, x_{\text{pa}(i)})}{n(x_{\text{pa}(i)})}$ |            å°æ•¸æ¢ä»¶æ©Ÿç‡            | æ¯å€‹äº‹ä»¶çµ„åˆçš„å°æ•¸æ©Ÿç‡è²¢ç»å€¼     |



### (OK) (42B P.35) How to count the dim(M) in scoring functions(AIC, BIC)?
solution of exercise 5 - question 3 - (C) è£¡é¢æœ‰è©³ç´°çš„è§£é‡‹ï¼š  

`Suppose a node has k different parent configurations (possible value assignments to its parents), and it can take on m different values itself. Then the number of parameters associated with that node is k(mâˆ’1) because you have to estimate k different conditional distributions, and each conditional distribution requires the estimation of mâˆ’1 probabilities.`  

ä¹Ÿå°±æ˜¯ï¼Œé€™å€‹nodeçš„æ‰€æœ‰parentå¯èƒ½çš„çµ„åˆ(k) ä¹˜ä»¥ é€™å€‹nodeå¯èƒ½çš„å€¼æ¸›ä¸€(m-1)ï¼Œå°±æ˜¯é€™å€‹modeçš„dimensionã€‚æœ€å¾Œé‚„è¦æŠŠæ‰€æœ‰çš„nodeçš„dimensionç›¸åŠ ã€‚å¾—åˆ°ä»¥ä¸‹å…¬å¼ï¼š

$$
\text{dim}(M) = \sum_{i=1}^{k} (d_i - 1) \prod_{X_j \in \text{pa}(X_i)} d_j
$$

| ç¬¦è™Ÿ | åç¨± | æ„æ€ |
| :-------: | :------ | :------ | 
|   $M$  | æ¨¡å‹  | æŒ‡é€™å€‹è²è‘‰æ–¯ç¶²è·¯æ¨¡å‹  |   |   |
| $k$ | è®Šé‡æ•¸é‡ | ç¶²è·¯ä¸­ç¯€é»ï¼ˆæˆ–éš¨æ©Ÿè®Šé‡ï¼‰çš„æ•¸ç›® |
| $X_i$ | ç¬¬ $i$ å€‹è®Šé‡ | ç¶²è·¯ä¸­çš„ä¸€å€‹éš¨æ©Ÿè®Šé‡ |
| $\text{pa}(X_i)$ | çˆ¶ç¯€é»é›†åˆ | åœ¨ DAG ä¸­ç›´æ¥æŒ‡å‘ $X_i$ çš„ç¯€é»ï¼ˆå³ $X_i$ çš„çˆ¶ç¯€é»ï¼‰ |
| $d_i$ | è®Šé‡ $X_i$ çš„å–å€¼æ•¸é‡ | è‹¥ $X_i$ æ˜¯é›¢æ•£è®Šé‡ï¼Œå‰‡ $d_i = \| \text{Val}(X_i) \|$ï¼Œä¾‹å¦‚äºŒå…ƒè®Šé‡ $d_i = 2$ |
| $\prod_{X_j \in \text{pa}(X_i)} d_j$ | çˆ¶ç¯€é»çµ„åˆçš„ç¸½æ•¸ | æ‰€æœ‰çˆ¶ç¯€é»çš„å–å€¼å¯èƒ½çµ„åˆæ•¸é‡ |
| $(d_i - 1)$ | æ¯å€‹æ¢ä»¶æ©Ÿç‡åˆ†å¸ƒéœ€ä¼°çš„è‡ªç”±åƒæ•¸æ•¸ | å› ç‚ºæ©Ÿç‡ç¸½å’Œç‚º 1ï¼Œæ‰€ä»¥å° ( d_i ) å€‹å€¼åªéœ€è¦ ( d_i - 1 ) å€‹è‡ªç”±åƒæ•¸ |
| $\text{dim}(M)$ | æ¨¡å‹çš„ç¶­åº¦æˆ–åƒæ•¸ç¸½æ•¸ | è²è‘‰æ–¯ç¶²ä¸­éœ€è¦ä¼°è¨ˆçš„æ‰€æœ‰æ¢ä»¶æ©Ÿç‡åƒæ•¸çš„æ•¸é‡ç¸½å’Œ |


### (OK) (42B P.42) **Mutual independence model** (=empty graph)
Mutual independence model means empty graph.

### (OK) (42B P.44) Markov Equivalence & V-Structures
1. Markov Equivalence:  
 Two DAGs are Markov equivalent if and only if
    1. they have the same skeleton (same undirected graph when you drop the directions of all edges), and
    2. they have the same immoralities (v-structures).

2. æ€æ¨£ç®—æ˜¯æœ‰ç›¸åŒçš„ v-structures  
    åœ¨ä¸€å€‹æœ‰å‘ç„¡ç’°åœ–ï¼ˆDAGï¼‰ä¸­ï¼Œä¸€å€‹ **v-structure**ï¼ˆä¹Ÿç¨± **collider çµæ§‹**ï¼‰æ˜¯é€™æ¨£çš„ä¸€ç¨®å±€éƒ¨å½¢ç‹€ï¼š
    $
    A \to C \leftarrow B
    $
    è€Œä¸” **A å’Œ B ä¹‹é–“æ²’æœ‰ä»»ä½•é‚Šç›¸é€£ï¼ˆä¸è«–æ–¹å‘ï¼‰**ã€‚


    1. èˆ‰ä¾‹ï¼šä»¥ä¸‹æ˜¯v-structure
        ```
        A â†’ C â† B
        ```
        é€™æ˜¯ v-structureï¼Œå› ç‚ºï¼š

        * A å’Œ B éƒ½æŒ‡å‘åŒä¸€å€‹ç¯€é» Cï¼›
        * A å’Œ B ä¹‹é–“ **æ²’æœ‰é‚Š**ã€‚


    2. èˆ‰ä¾‹ï¼šä»¥ä¸‹*ä¸*æ˜¯v-structure
        ```
        A â†’ C â†’ B
        ```
        ```
        A â† C â† B
        ```
        ```
        A â†’ C â† Bï¼Œä½† A â†” B æœ‰é‚Š
        ```
        ï¼ˆå› ç‚º A å’Œ B ä¹‹é–“æœ‰é€£çµï¼Œæ‰€ä»¥ä¸ç®—ã€Œvã€çµæ§‹ï¼‰

3. åˆ¤æ–·å…©å€‹åœ–æ˜¯å¦ç­‰åƒ¹
    1. é€™å…©å€‹åœ–æ˜¯ç­‰åƒ¹çš„
        ```
        G1:  A â†’ B â†’ C
        G2:  A â† B â†’ C
        ```  
        * Skeleton éƒ½æ˜¯ä¸€æ¨£çš„ï¼šAâ€”Bâ€”C âœ…
        * çœ‹ v-structureï¼š
            * G1 æ²’æœ‰ v-structureï¼ˆå› ç‚ºæ²’æœ‰ç¯€é»åŒæ™‚æœ‰å…©å€‹çˆ¶ï¼‰
            * G2 ä¹Ÿæ²’æœ‰ v-structure âœ…
                â†’ å› æ­¤ G1 èˆ‡ G2 **ç­‰åƒ¹**ã€‚

    2. é€™å…©å€‹åœ–ä¸ç­‰åƒ¹
        ```
        G3:  A â†’ C â† B
        G4:  A â†’ C â†’ B
        ```
        * Skeleton ä¸€æ¨£ï¼ˆAâ€“Câ€“Bï¼‰âœ…
        * ä½†ï¼š
            * G3 æœ‰ä¸€å€‹ v-structureï¼šAâ†’Câ†B âœ…
            * G4 æ²’æœ‰ v-structure âŒ
                â†’ æ‰€ä»¥å®ƒå€‘ **ä¸ç­‰åƒ¹**ã€‚

### (OK) (42B P.44) Essential Graph
For a given DAG, an edge becomes bi-directional in the essential graph if there is an equivalent DAG in which the direction of the edge is reversed.

- ç¯„ä¾‹ç¿’é¡Œ
exercise 4 - question 5 çµ¦ä¸€å€‹ DAGï¼Œè¦ä½ æ‰¾å‡ºä»–çš„ essential graphã€‚

### (OK) (42B P.36,42)Hill-Climbing Algorithm æœ‰å“ªäº›è¦å‰‡?  
ç¯„ä¾‹ç¿’é¡Œï¼šExc 5 - Qusestion 3 - d
1. **å¿…é ˆä¿æŒ DAGï¼ˆæœ‰å‘ç„¡ç’°åœ–ï¼‰**
   â€“ ç„¡è«–æ˜¯æ–°å¢ã€åˆªé™¤æˆ–åè½‰é‚Šï¼Œæœ€å¾Œå¾—åˆ°çš„åœ–éƒ½å¿…é ˆæ²’æœ‰æœ‰å‘ç’°ï¼ˆcycleï¼‰  
   â€“ å› æ­¤ï¼š  
    - æ–°å¢ä¸€æ¢é‚Š $i\to j$ å‰ï¼Œè¦æª¢æŸ¥æ˜¯å¦æœƒå½¢æˆå¾ $j$ å›åˆ° $i$ çš„æœ‰å‘è·¯å¾‘ï¼ˆè‹¥æœƒï¼Œç¦æ­¢æ–°å¢ï¼‰ã€‚  
    - åè½‰ä¸€æ¢å·²å­˜åœ¨çš„é‚Š $i\to j$ï¼ˆè®Šæˆ $j\to i$å‰)ï¼Œä¹Ÿè¦æª¢æŸ¥åè½‰å¾Œæ˜¯å¦æœƒé€ æˆç’°ï¼ˆè‹¥æœƒï¼Œç¦æ­¢åè½‰ï¼‰ã€‚

2. **ä¸èƒ½å»ºç«‹é‡è¤‡é‚Šæˆ–è‡ªç’°**
   â€“ ä¸èƒ½åŠ å·²å­˜åœ¨çš„æœ‰å‘é‚Šæˆ–åŠ  $i\to i$ï¼ˆè‡ªç’°ï¼‰ã€‚
   â€“ åˆªé™¤åªèƒ½å°å·²å­˜åœ¨çš„é‚Šåšï¼ˆä¸èƒ½åˆªä¸å­˜åœ¨çš„é‚Šï¼‰ã€‚
   â€“ åè½‰åªèƒ½å°å·²å­˜åœ¨çš„é‚Šåšï¼ˆä¸èƒ½å°ä¸å­˜åœ¨çš„é‚Šåè½‰ï¼‰ã€‚

3. **å–®æ¬¡é„°å±…æ“ä½œé€šå¸¸åªæ”¹å‹•ä¸€æ¢é‚Š**
   â€“ åœ¨ hill-climbing / å±€éƒ¨æœå°‹è£¡ï¼Œä¸€å€‹ neighbour é€šå¸¸æ˜¯å°ç›®å‰åœ–åšã€Œæ–°å¢ä¸€æ¢é‚Šã€æˆ–ã€Œåˆªé™¤ä¸€æ¢é‚Šã€æˆ–ã€Œåè½‰ä¸€æ¢é‚Šã€å…¶ä¸­ä¸€ç¨®ã€åªæ”¹ä¸€æ¢é‚Šã€‚

èˆ‰ä¾‹ï¼šexercise 5 - question 5 æœƒæœ‰ç¿’é¡Œå•èªªï¼šå¦‚æœé€™ä¸€æ­¥é©Ÿåšäº†æŸå€‹å‹•ä½œï¼ˆæ–°å¢ã€åˆªé™¤ã€åè½‰ï¼‰ï¼Œä¸‹ä¸€æ­¥èµ°çš„scoreæœ‰å“ªäº›éœ€è¦é‡ç®—ï¼Ÿ  

å› ç‚ºscoreåªå–æ±ºæ–¼nodeçš„parentsé›†åˆï¼Œæ‰€ä»¥åªéœ€è¦é‡æ–°è¨ˆç®—å°é‚£å€‹nodeçš„æ‰€æœ‰å‹•ä½œå°±å¥½ã€‚ex: add(?? -> node), remove(?? -> node), reverse(?? -> node)  

åªè¦é‚£å€‹æ“ä½œæ”¹è®Šäº†æŸå€‹ node çš„ parentsï¼Œå°±è¦é‡ç®—æ‰€æœ‰ã€Œé€£åˆ°é‚£å€‹ nodeã€çš„ add / remove / reverse çš„åˆ†æ•¸ã€‚

## Doubt
### Exc 5
#### Exc 5 - Question 2 - e
- ç‚ºä½• Î”dim=1

### HW3
#### HW3 - Question 3 - C

### HW4
#### HW4 - Question 4